{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorflow version: 1.12.0-rc0\n",
    "scikit-learn version: 0.17\n",
    "keras version: 2.2.4\n",
    "tensorboard version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "In this assignment, we will use the weights of a network pre-trained in a particular problem as starting point to train our CNN to a different problem. As training a network from scratch is time-consuming and demands a lot of data, this is a frequent strategy, specially if both datasets (the one used for pre-training and the target) shares similar structures/elements/concepts. \n",
    "\n",
    "This is specially true when working with images. Most filters learned in initial convolutional layers will detect low-level elements, such as borders, corners and color blobs, which are common to most problems in the image domain. \n",
    "\n",
    "In this notebook, we will load the SqueezeNet architecture trained in the ImageNet dataset and fine-tune it to CIFAR-10.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import sample, seed\n",
    "\n",
    "from time import time\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Utility to plot\n",
    "def plotImages(imgList):\n",
    "    for i in range(len(imgList)):\n",
    "        plotImage(imgList[i])\n",
    "        \n",
    "        \n",
    "def plotImage(img):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.imshow(np.uint8(img), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeNet definition\n",
    "These methods define our architecture and load the weights obtained using ImageNet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire Module Definition\n",
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "#SqueezeNet model definition\n",
    "def SqueezeNet(input_shape):\n",
    "    img_input = Input(shape=input_shape) #placeholder\n",
    "    \n",
    "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    \n",
    "    x = Dropout(0.5, name='drop9')(x)\n",
    "\n",
    "    x = Convolution2D(1000, (1, 1), padding='valid', name='conv10')(x)\n",
    "    x = Activation('relu', name='relu_conv10')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='squeezenet')\n",
    "\n",
    "    # Download and load ImageNet weights\n",
    "    model.load_weights('./squeezenet_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The class are **airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val data. X:  (50000, 32, 32, 3) , Y:  (50000, 32, 32, 3)\n",
      "Test data. X:  (10000, 32, 32, 3) , Y:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = cifar10.load_data()\n",
    "print(\"Train/Val data. X: \", trainVal_data.shape, \", Y: \", trainVal_data.shape)\n",
    "print(\"Test data. X: \", X_test.shape, \", Y: \", y_test.shape)\n",
    "\n",
    "# Prepare the data\n",
    "# ...\n",
    "train_x, val_x, train_labels, val_labels = train_test_split(trainVal_data, trainVal_label, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "train_categorical_labels = to_categorical(train_labels, num_classes=None)\n",
    "val_categorical_labels = to_categorical(val_labels, num_classes=None)\n",
    "test_categorical_labels = to_categorical(y_test, num_classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## SqueezeNet with frozen layers\n",
    "Our initial attempt will be to remove SqueezeNet's top layers --- responsible for the classification into ImageNet classes --- and train a new set of layers to our CIFAR-10 classes. We will also freeze the layers before `drop9`. Our architecture will be like this:\n",
    "\n",
    "<img src=\"frozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "\n",
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "#freeze layers\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Add new classification layers\n",
    "x = squeezeNetModel.layers[-5].output\n",
    "x = Convolution2D(10, (1, 1), padding='valid', name='new_conv10')(x)\n",
    "x = Activation('relu', name='new_relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='new_loss')(x)\n",
    "\n",
    "#new Model\n",
    "model = Model(squeezeNetModel.inputs, x, name='squeezenet_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-4].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 2.3217 - acc: 0.1207 - val_loss: 2.2565 - val_acc: 0.2040\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 2.2511 - acc: 0.1792 - val_loss: 2.1974 - val_acc: 0.2538\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 2.2089 - acc: 0.2162 - val_loss: 2.1481 - val_acc: 0.2752\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 2.1755 - acc: 0.2383 - val_loss: 2.1130 - val_acc: 0.2868\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 2.1494 - acc: 0.2481 - val_loss: 2.0872 - val_acc: 0.3003\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 2.1220 - acc: 0.2626 - val_loss: 2.0573 - val_acc: 0.3121\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 2.0879 - acc: 0.2699 - val_loss: 1.9848 - val_acc: 0.3272\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 2.0401 - acc: 0.2785 - val_loss: 1.9363 - val_acc: 0.3469\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 2.0075 - acc: 0.2873 - val_loss: 1.9095 - val_acc: 0.3592\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.9899 - acc: 0.2972 - val_loss: 1.8933 - val_acc: 0.3630\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.9715 - acc: 0.3038 - val_loss: 1.8791 - val_acc: 0.3751\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.9642 - acc: 0.3089 - val_loss: 1.8676 - val_acc: 0.3749\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.9500 - acc: 0.3170 - val_loss: 1.8593 - val_acc: 0.3843\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.9424 - acc: 0.3187 - val_loss: 1.8560 - val_acc: 0.3814\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.9153 - acc: 0.3243 - val_loss: 1.8105 - val_acc: 0.3853\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8967 - acc: 0.3268 - val_loss: 1.7932 - val_acc: 0.3924\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8800 - acc: 0.3302 - val_loss: 1.7794 - val_acc: 0.3968\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8670 - acc: 0.3343 - val_loss: 1.7695 - val_acc: 0.4006\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8566 - acc: 0.3392 - val_loss: 1.7588 - val_acc: 0.3985\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8510 - acc: 0.3436 - val_loss: 1.7520 - val_acc: 0.4013\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8490 - acc: 0.3386 - val_loss: 1.7474 - val_acc: 0.4042\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8399 - acc: 0.3406 - val_loss: 1.7392 - val_acc: 0.4058\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8398 - acc: 0.3446 - val_loss: 1.7392 - val_acc: 0.4061\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8361 - acc: 0.3447 - val_loss: 1.7331 - val_acc: 0.4102\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8290 - acc: 0.3478 - val_loss: 1.7265 - val_acc: 0.4109\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8241 - acc: 0.3478 - val_loss: 1.7248 - val_acc: 0.4064\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8201 - acc: 0.3497 - val_loss: 1.7210 - val_acc: 0.4080\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8223 - acc: 0.3503 - val_loss: 1.7150 - val_acc: 0.4175\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8184 - acc: 0.3497 - val_loss: 1.7124 - val_acc: 0.4075\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.8159 - acc: 0.3520 - val_loss: 1.7106 - val_acc: 0.4132\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8120 - acc: 0.3525 - val_loss: 1.7047 - val_acc: 0.4154\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8094 - acc: 0.3533 - val_loss: 1.7081 - val_acc: 0.4139\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8055 - acc: 0.3569 - val_loss: 1.7030 - val_acc: 0.4188\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8054 - acc: 0.3565 - val_loss: 1.7015 - val_acc: 0.4186\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8026 - acc: 0.3585 - val_loss: 1.6992 - val_acc: 0.4140\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8025 - acc: 0.3569 - val_loss: 1.6964 - val_acc: 0.4175\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8063 - acc: 0.3581 - val_loss: 1.6983 - val_acc: 0.4146\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7977 - acc: 0.3579 - val_loss: 1.6954 - val_acc: 0.4179\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7997 - acc: 0.3583 - val_loss: 1.6911 - val_acc: 0.4220\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7945 - acc: 0.3621 - val_loss: 1.6901 - val_acc: 0.4213\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7956 - acc: 0.3616 - val_loss: 1.6891 - val_acc: 0.4154\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7975 - acc: 0.3557 - val_loss: 1.6906 - val_acc: 0.4210\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7961 - acc: 0.3589 - val_loss: 1.6878 - val_acc: 0.4227\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7898 - acc: 0.3627 - val_loss: 1.6866 - val_acc: 0.4206\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7874 - acc: 0.3604 - val_loss: 1.6881 - val_acc: 0.4202\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7895 - acc: 0.3603 - val_loss: 1.6838 - val_acc: 0.4193\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7871 - acc: 0.3645 - val_loss: 1.6795 - val_acc: 0.4204\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7843 - acc: 0.3631 - val_loss: 1.6834 - val_acc: 0.4182\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7875 - acc: 0.3612 - val_loss: 1.6831 - val_acc: 0.4207\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7867 - acc: 0.3631 - val_loss: 1.6792 - val_acc: 0.4228\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7814 - acc: 0.3634 - val_loss: 1.6771 - val_acc: 0.4215\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7837 - acc: 0.3630 - val_loss: 1.6750 - val_acc: 0.4211\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7864 - acc: 0.3604 - val_loss: 1.6772 - val_acc: 0.4226\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7863 - acc: 0.3629 - val_loss: 1.6720 - val_acc: 0.4256\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7816 - acc: 0.3658 - val_loss: 1.6750 - val_acc: 0.4216\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7781 - acc: 0.3684 - val_loss: 1.6717 - val_acc: 0.4282\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7759 - acc: 0.3674 - val_loss: 1.6715 - val_acc: 0.4239\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7778 - acc: 0.3670 - val_loss: 1.6717 - val_acc: 0.4267\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7728 - acc: 0.3656 - val_loss: 1.6658 - val_acc: 0.4258\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7783 - acc: 0.3656 - val_loss: 1.6728 - val_acc: 0.4206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7753 - acc: 0.3662 - val_loss: 1.6680 - val_acc: 0.4234\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.7751 - acc: 0.3676 - val_loss: 1.6688 - val_acc: 0.4213\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "BATCHSIZE = 32\n",
    "\n",
    "#Compile model\n",
    "# ...\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr = 0.0001, momentum=0.9, nesterov=True), metrics=[\"accuracy\"])\n",
    "\n",
    "#Tensorboard callback\n",
    "#tbCallBack = TensorBoard(log_dir=\"./logs/rafa\", write_graph=True)\n",
    "tbCallBacks = [callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3,\n",
    "                                      verbose=0, mode='auto', baseline=None, \n",
    "                                      restore_best_weights=False),\n",
    "              TensorBoard(log_dir=\"./monitor/1\".format(time()), write_graph=True)]\n",
    "    \n",
    "#Train model\n",
    "# ...\n",
    "\n",
    "history = model.fit_generator(datagen.flow(train_x, train_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_data=datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_steps=len(val_x)/BATCHSIZE,\n",
    "                    steps_per_epoch=len(train_x)/BATCHSIZE, \n",
    "                    epochs=100, use_multiprocessing=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=tbCallBacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[1.6702335821151733, 0.4243]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation:\n",
    "# ...\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate_generator(datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE), steps=len(val_x)/BATCHSIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "-----------------\n",
    "\n",
    "# Training last 2 Fire Modules + classification layers\n",
    "As we could see, the frozen network performed very poorly. By freezing most layers, we do not allow SqueezeNet to adapt its weights to features present in CIFAR-10.\n",
    "\n",
    "Let's try to unfreeze the last two fire modules and train once more. The architecture will be:\n",
    "<img src=\"partFrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "#freeze the mentioned layers\n",
    "# ...\n",
    "for layer in squeezeNetModel.layers[:-19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Add new classification layers\n",
    "# ...\n",
    "x = squeezeNetModel.layers[-5].output\n",
    "x = Convolution2D(10, (1, 1), padding='valid', name='new_conv10')(x)\n",
    "x = Activation('relu', name='new_relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='new_loss')(x)\n",
    "\n",
    "#new Model\n",
    "model = Model(squeezeNetModel.inputs, x, name='squeezenet_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "new_conv10 (Conv2D)             (None, 1, 1, 10)     5130        drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "new_relu_conv10 (Activation)    (None, 1, 1, 10)     0           new_conv10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 10)           0           new_relu_conv10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "new_loss (Activation)           (None, 10)           0           global_average_pooling2d_6[0][0] \n",
      "==================================================================================================\n",
      "Total params: 727,626\n",
      "Trainable params: 391,306\n",
      "Non-trainable params: 336,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-20].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 2.2418 - acc: 0.1782 - val_loss: 2.1159 - val_acc: 0.2698\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 2.0911 - acc: 0.2592 - val_loss: 1.9646 - val_acc: 0.3393\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.9207 - acc: 0.3218 - val_loss: 1.7347 - val_acc: 0.4030\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.7778 - acc: 0.3603 - val_loss: 1.6350 - val_acc: 0.4200\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.7081 - acc: 0.3830 - val_loss: 1.5868 - val_acc: 0.4387\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.6621 - acc: 0.4025 - val_loss: 1.5483 - val_acc: 0.4472\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.6287 - acc: 0.4166 - val_loss: 1.5282 - val_acc: 0.4532\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.6024 - acc: 0.4273 - val_loss: 1.5124 - val_acc: 0.4643\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.5797 - acc: 0.4332 - val_loss: 1.4914 - val_acc: 0.4725\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.5647 - acc: 0.4405 - val_loss: 1.4787 - val_acc: 0.4734\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.5532 - acc: 0.4451 - val_loss: 1.4636 - val_acc: 0.4760\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.5414 - acc: 0.4489 - val_loss: 1.4610 - val_acc: 0.4806\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.5271 - acc: 0.4603 - val_loss: 1.4507 - val_acc: 0.4837\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.5158 - acc: 0.4584 - val_loss: 1.4437 - val_acc: 0.4869\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.5089 - acc: 0.4645 - val_loss: 1.4357 - val_acc: 0.4943\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.4981 - acc: 0.4695 - val_loss: 1.4339 - val_acc: 0.4873\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.4948 - acc: 0.4691 - val_loss: 1.4217 - val_acc: 0.4935\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.4858 - acc: 0.4719 - val_loss: 1.4227 - val_acc: 0.4934\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4778 - acc: 0.4777 - val_loss: 1.4126 - val_acc: 0.4983\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4717 - acc: 0.4793 - val_loss: 1.4057 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4693 - acc: 0.4770 - val_loss: 1.3975 - val_acc: 0.5011\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4646 - acc: 0.4818 - val_loss: 1.3977 - val_acc: 0.5037\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4601 - acc: 0.4836 - val_loss: 1.3881 - val_acc: 0.5068\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.4529 - acc: 0.4855 - val_loss: 1.3902 - val_acc: 0.5070\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4497 - acc: 0.4882 - val_loss: 1.3858 - val_acc: 0.5068\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4480 - acc: 0.4874 - val_loss: 1.3845 - val_acc: 0.5070\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4417 - acc: 0.4885 - val_loss: 1.3825 - val_acc: 0.5101\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.4365 - acc: 0.4907 - val_loss: 1.3765 - val_acc: 0.5075\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4378 - acc: 0.4917 - val_loss: 1.3784 - val_acc: 0.5100\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4295 - acc: 0.4958 - val_loss: 1.3764 - val_acc: 0.5109\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4276 - acc: 0.4913 - val_loss: 1.3690 - val_acc: 0.5160\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4262 - acc: 0.4938 - val_loss: 1.3686 - val_acc: 0.5134\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.4200 - acc: 0.4986 - val_loss: 1.3663 - val_acc: 0.5140\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4181 - acc: 0.4968 - val_loss: 1.3633 - val_acc: 0.5156\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.4141 - acc: 0.4970 - val_loss: 1.3622 - val_acc: 0.5129\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4132 - acc: 0.5018 - val_loss: 1.3549 - val_acc: 0.5208\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4151 - acc: 0.4981 - val_loss: 1.3549 - val_acc: 0.5173\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4088 - acc: 0.5006 - val_loss: 1.3515 - val_acc: 0.5181\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4061 - acc: 0.5019 - val_loss: 1.3598 - val_acc: 0.5168\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4070 - acc: 0.5031 - val_loss: 1.3471 - val_acc: 0.5228\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.3968 - acc: 0.5041 - val_loss: 1.3528 - val_acc: 0.5175\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3978 - acc: 0.5042 - val_loss: 1.3440 - val_acc: 0.5256\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3975 - acc: 0.5050 - val_loss: 1.3439 - val_acc: 0.5240\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3956 - acc: 0.5036 - val_loss: 1.3432 - val_acc: 0.5237\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3954 - acc: 0.5079 - val_loss: 1.3386 - val_acc: 0.5267\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3916 - acc: 0.5055 - val_loss: 1.3445 - val_acc: 0.5222\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3882 - acc: 0.5089 - val_loss: 1.3403 - val_acc: 0.5191\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3904 - acc: 0.5080 - val_loss: 1.3373 - val_acc: 0.5243\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3862 - acc: 0.5097 - val_loss: 1.3309 - val_acc: 0.5281\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3865 - acc: 0.5090 - val_loss: 1.3341 - val_acc: 0.5273\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3794 - acc: 0.5149 - val_loss: 1.3261 - val_acc: 0.5338\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3828 - acc: 0.5092 - val_loss: 1.3303 - val_acc: 0.5291\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3775 - acc: 0.5098 - val_loss: 1.3285 - val_acc: 0.5311\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3765 - acc: 0.5124 - val_loss: 1.3254 - val_acc: 0.5320\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3739 - acc: 0.5142 - val_loss: 1.3302 - val_acc: 0.5264\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.3716 - acc: 0.5143 - val_loss: 1.3269 - val_acc: 0.5316\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3730 - acc: 0.5142 - val_loss: 1.3224 - val_acc: 0.5347\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3699 - acc: 0.5145 - val_loss: 1.3315 - val_acc: 0.5316\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3677 - acc: 0.5149 - val_loss: 1.3224 - val_acc: 0.5373\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.3688 - acc: 0.5153 - val_loss: 1.3320 - val_acc: 0.5307\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3630 - acc: 0.5164 - val_loss: 1.3190 - val_acc: 0.5341\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.3678 - acc: 0.5179 - val_loss: 1.3148 - val_acc: 0.5374\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3609 - acc: 0.5166 - val_loss: 1.3231 - val_acc: 0.5341\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3630 - acc: 0.5164 - val_loss: 1.3134 - val_acc: 0.5383\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3582 - acc: 0.5186 - val_loss: 1.3079 - val_acc: 0.5366\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3623 - acc: 0.5185 - val_loss: 1.3112 - val_acc: 0.5386\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3558 - acc: 0.5172 - val_loss: 1.3167 - val_acc: 0.5332\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3564 - acc: 0.5202 - val_loss: 1.3084 - val_acc: 0.5378\n"
     ]
    }
   ],
   "source": [
    "#Compile model and train it\n",
    "# ...\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen.fit(train_x)\n",
    "\n",
    "BATCHSIZE = 32\n",
    "\n",
    "tbCallBacks = [callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3,\n",
    "                                      verbose=0, mode='auto', baseline=None, \n",
    "                                      restore_best_weights=False),\n",
    "              TensorBoard(log_dir=\"./monitor/2\".format(time()), write_graph=True)]\n",
    "\n",
    "#Compile model\n",
    "# ...\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr = 0.0001, momentum=0.9, nesterov=True), metrics=[\"accuracy\"])\n",
    "\n",
    "#Train model\n",
    "# ...\n",
    "\n",
    "history = model.fit_generator(datagen.flow(train_x, train_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_data=datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_steps=len(val_x)/BATCHSIZE,\n",
    "                    steps_per_epoch=len(train_x)/BATCHSIZE, \n",
    "                    epochs=100, use_multiprocessing=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=tbCallBacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[1.3119075336456298, 0.5364]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation:\n",
    "# ...\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate_generator(datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE), steps=len(val_x)/BATCHSIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "# Tensorboard\n",
    "\n",
    "Tensorboard is a visualization tool for Tensorflow. Among other things, it allows us to monitor the progress of our training, plot metrics per epochs, visualize the architecture's schematics. \n",
    "\n",
    "Just like for Early Stopping, we will use the [Tensorboard callback](https://keras.io/callbacks/#tensorboard) to log the information about our training. An example of usage, would be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Just an example, DON'T RUN! \n",
    "### You will need to change <<LOG_DIR>>\n",
    "import keras.callbacks as callbacks\n",
    "tbCallBack = callbacks.TensorBoard(log_dir = \"./<<LOG_DIR>>\")\n",
    "model.fit(..., callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your training progresses, Keras will log the metrics (e.g., loss, accuracy) to `<<LOG_DIR>>` (**make sure `<<LOG_DIR>>` is a valid directory)**. On your terminal, you will need to run Tensorboard, assign a port and access it via browser (just like jupyter).\n",
    "\n",
    "#### ----> MAKE SURE YOU USE A DIFFERENT PORT FOR JUPYTER AND TENSORBOARD <----\n",
    "\n",
    "### Docker\n",
    "For those using docker, open a new terminal and create a new container (using the same image) running Tensorboard:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p <<port_host>>:<<port_container>>\n",
    "            --volume=<<LOG_DIR>>:<<LOG_DIR>>\n",
    "            --name=<<container_name>> <<docker_image>> \n",
    "            tensorboard --logdir=<<LOG_DIR>> --port=<<port_container>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p 8887:8887\n",
    "            --volume=/your/path/ml2018/:/ml2018\n",
    "            --name=mdc_container_tensorboard mdc-keras:cpu\n",
    "            tensorboard --logdir=/ml2018/logs --port=8887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port_container>>`.\n",
    "\n",
    "### Anaconda\n",
    "$ tensorboard --logdir=<<LOG_DIR>> --port=<<port>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port>>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "\n",
    "# Fine-tuning all layers\n",
    "\n",
    "What if we fine-tune all layers of SqueezeNet?\n",
    "<img src=\"unfrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = True       #by default they are all trainable, but just for clarification\n",
    "\n",
    "#Add new classification layers\n",
    "# ...\n",
    "x = squeezeNetModel.layers[-5].output\n",
    "x = Convolution2D(10, (1, 1), padding='valid', name='new_conv10')(x)\n",
    "x = Activation('relu', name='new_relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='new_loss')(x)\n",
    "\n",
    "#new Model\n",
    "model_adam = Model(squeezeNetModel.inputs, x, name='squeezenet_new_adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 1.9475 - acc: 0.2585 - val_loss: 1.5548 - val_acc: 0.4341\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 1.3286 - acc: 0.5306 - val_loss: 1.1291 - val_acc: 0.6053\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 28s 23ms/step - loss: 1.1056 - acc: 0.6197 - val_loss: 1.0364 - val_acc: 0.6522\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 1.0030 - acc: 0.6621 - val_loss: 0.9705 - val_acc: 0.6728\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.9429 - acc: 0.6848 - val_loss: 0.9234 - val_acc: 0.6957\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.8914 - acc: 0.7050 - val_loss: 0.9056 - val_acc: 0.6971\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.8472 - acc: 0.7200 - val_loss: 0.8743 - val_acc: 0.7071\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.8181 - acc: 0.7310 - val_loss: 0.8784 - val_acc: 0.7086\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.7827 - acc: 0.7429 - val_loss: 0.8144 - val_acc: 0.7361\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 28s 23ms/step - loss: 0.7673 - acc: 0.7472 - val_loss: 0.8248 - val_acc: 0.7409\n",
      "Epoch 11/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.7483 - acc: 0.7534"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import keras.callbacks as callbacks\n",
    "BATCHSIZE = 32\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen.fit(train_x)\n",
    "\n",
    "#Compile model\n",
    "# ...\n",
    "model_adam.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(), metrics=[\"accuracy\"])\n",
    "\n",
    "#Tensorboard callback\n",
    "\n",
    "tbCallBacks = [callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3,\n",
    "                                      verbose=0, mode='auto', baseline=None, \n",
    "                                      restore_best_weights=False),\n",
    "              TensorBoard(log_dir=\"./monitor/3/adam\".format(time()), write_graph=True)]\n",
    "\n",
    "#Train model\n",
    "# ...\n",
    "\n",
    "history = model_adam.fit_generator(datagen.flow(train_x, train_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_data=datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_steps=len(val_x)/BATCHSIZE,\n",
    "                    steps_per_epoch=len(train_x)/BATCHSIZE, \n",
    "                    epochs=100, use_multiprocessing=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=tbCallBacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = True       #by default they are all trainable, but just for clarification\n",
    "\n",
    "#Add new classification layers\n",
    "# ...\n",
    "x = squeezeNetModel.layers[-5].output\n",
    "x = Convolution2D(10, (1, 1), padding='valid', name='new_conv10')(x)\n",
    "x = Activation('relu', name='new_relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='new_loss')(x)\n",
    "\n",
    "#new Model\n",
    "model_sgd = Model(squeezeNetModel.inputs, x, name='squeezenet_new_sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "BATCHSIZE=32\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen.fit(train_x)\n",
    "\n",
    "#Compile model\n",
    "# ...\n",
    "model_sgd.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr = 0.0001, momentum=0.9, nesterov=True), metrics=[\"accuracy\"])\n",
    "\n",
    "#Tensorboard callback\n",
    "tbCallBacks = [callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3,\n",
    "                                      verbose=0, mode='auto', baseline=None, \n",
    "                                      restore_best_weights=False),\n",
    "              TensorBoard(log_dir=\"./monitor/3/sgd\".format(time()), write_graph=True)]\n",
    "\n",
    "#Train model\n",
    "# ...\n",
    "history = model_sgd.fit_generator(datagen.flow(train_x, train_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_data=datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE),\n",
    "                    validation_steps=len(val_x)/BATCHSIZE,\n",
    "                    steps_per_epoch=len(train_x)/BATCHSIZE, \n",
    "                    epochs=100, use_multiprocessing=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=tbCallBacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation\n",
    "# ...\n",
    "\n",
    "print(model_sgd.metrics_names)\n",
    "print(model_sgd.evaluate_generator(datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE), steps=len(val_x)/BATCHSIZE))\n",
    "\n",
    "print(model_adam.metrics_names)\n",
    "print(model_adam.evaluate_generator(datagen.flow(val_x, val_categorical_labels, batch_size=BATCHSIZE), steps=len(val_x)/BATCHSIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your best model on test\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "Now that we are working on more complex tasks and our trainings are starting to take more time it is usually a good idea to save the trained model from time to time. [Keras has a lot of ways of saving and loading the model](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model), but in this exercise we will use the simplest of them all: `model.save()`. It saves the architecture, the weights, the choice of loss function/optimizer/metrics and even the current state of the training, so you can resume your training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model\n",
    "Once we have our model trained, we can load it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "del model  # Will delete model, only to check if load_model is working\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# evaluate test set again... should give us the same result\n",
    "# ...\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NORMALIZED):', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
