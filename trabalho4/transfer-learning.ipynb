{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorflow version: 1.12.0-rc0\n",
    "scikit-learn version: 0.17\n",
    "keras version: 2.2.4\n",
    "tensorboard version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "In this assignment, we will use the weights of a network pre-trained in a particular problem as starting point to train our CNN to a different problem. As training a network from scratch is time-consuming and demands a lot of data, this is a frequent strategy, specially if both datasets (the one used for pre-training and the target) shares similar structures/elements/concepts. \n",
    "\n",
    "This is specially true when working with images. Most filters learned in initial convolutional layers will detect low-level elements, such as borders, corners and color blobs, which are common to most problems in the image domain. \n",
    "\n",
    "In this notebook, we will load the SqueezeNet architecture trained in the ImageNet dataset and fine-tune it to CIFAR-10.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import sample, seed\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "#Utility to plot\n",
    "def plotImages(imgList):\n",
    "    for i in range(len(imgList)):\n",
    "        plotImage(imgList[i])\n",
    "        \n",
    "        \n",
    "def plotImage(img):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.imshow(np.uint8(img), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeNet definition\n",
    "These methods define our architecture and load the weights obtained using ImageNet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire Module Definition\n",
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "#SqueezeNet model definition\n",
    "def SqueezeNet(input_shape):\n",
    "    img_input = Input(shape=input_shape) #placeholder\n",
    "    \n",
    "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    \n",
    "    x = Dropout(0.5, name='drop9')(x)\n",
    "\n",
    "    x = Convolution2D(1000, (1, 1), padding='valid', name='conv10')(x)\n",
    "    x = Activation('relu', name='relu_conv10')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='squeezenet')\n",
    "\n",
    "    # Download and load ImageNet weights\n",
    "    model.load_weights('./squeezenet_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The class are **airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val data. X:  (50000, 32, 32, 3) , Y:  (50000, 32, 32, 3)\n",
      "Test data. X:  (10000, 32, 32, 3) , Y:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = cifar10.load_data()\n",
    "print(\"Train/Val data. X: \", trainVal_data.shape, \", Y: \", trainVal_data.shape)\n",
    "print(\"Test data. X: \", X_test.shape, \", Y: \", y_test.shape)\n",
    "\n",
    "# Prepare the data\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## SqueezeNet with frozen layers\n",
    "Our initial attempt will be to remove SqueezeNet's top layers --- responsible for the classification into ImageNet classes --- and train a new set of layers to our CIFAR-10 classes. We will also freeze the layers before `drop9`. Our architecture will be like this:\n",
    "\n",
    "<img src=\"frozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "\n",
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "#freeze layers\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Add new classification layers\n",
    "x = squeezeNetModel.layers[-5].output\n",
    "x = Convolution2D(1000, (1, 1), padding='valid', name='new_conv10')(x)\n",
    "x = Activation('relu', name='new_relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='new_loss')(x)\n",
    "\n",
    "#new Model\n",
    "model = Model(squeezeNetModel.inputs, x, name='squeezenet_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeezeNetModel.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "# squeezeNetModel.fit(trainVal_data, trainVal_label, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1000)\n",
      "(?, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'drop9'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(squeezeNetModel.output.shape)\n",
    "print(model.output.shape)\n",
    "squeezeNetModel.layers[-5].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-4].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 29s 724us/step - loss: 3.3498 - acc: 0.2344 - val_loss: 2.0628 - val_acc: 0.2935\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 28s 712us/step - loss: 2.3825 - acc: 0.2520 - val_loss: 1.9704 - val_acc: 0.3187\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 29s 725us/step - loss: 2.3684 - acc: 0.2586 - val_loss: 2.0054 - val_acc: 0.3112\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 30s 754us/step - loss: 2.3386 - acc: 0.2557 - val_loss: 2.0672 - val_acc: 0.3109\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 29s 737us/step - loss: 2.3564 - acc: 0.2557 - val_loss: 1.9380 - val_acc: 0.3158\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 30s 747us/step - loss: 2.3692 - acc: 0.2574 - val_loss: 2.0061 - val_acc: 0.3008\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 30s 744us/step - loss: 2.3334 - acc: 0.2576 - val_loss: 1.9350 - val_acc: 0.3033\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 29s 729us/step - loss: 2.3175 - acc: 0.2600 - val_loss: 1.9850 - val_acc: 0.3102\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 30s 747us/step - loss: 2.3283 - acc: 0.2559 - val_loss: 1.9981 - val_acc: 0.3091\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 30s 752us/step - loss: 2.3182 - acc: 0.2577 - val_loss: 2.0258 - val_acc: 0.2780\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 30s 759us/step - loss: 2.3389 - acc: 0.2594 - val_loss: 1.9973 - val_acc: 0.3196\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 30s 751us/step - loss: 2.3449 - acc: 0.2567 - val_loss: 2.0481 - val_acc: 0.2785\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 30s 755us/step - loss: 2.3291 - acc: 0.2592 - val_loss: 2.0539 - val_acc: 0.2861\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 30s 751us/step - loss: 2.3191 - acc: 0.2638 - val_loss: 2.0032 - val_acc: 0.2807\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 30s 753us/step - loss: 2.3194 - acc: 0.2576 - val_loss: 1.9474 - val_acc: 0.3177\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 30s 756us/step - loss: 2.3141 - acc: 0.2644 - val_loss: 2.0320 - val_acc: 0.3068\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 31s 763us/step - loss: 2.3154 - acc: 0.2612 - val_loss: 1.9102 - val_acc: 0.3150\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 29s 733us/step - loss: 2.3256 - acc: 0.2623 - val_loss: 1.9113 - val_acc: 0.3142\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 30s 756us/step - loss: 2.3162 - acc: 0.2609 - val_loss: 1.9933 - val_acc: 0.2784\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 30s 758us/step - loss: 2.3216 - acc: 0.2590 - val_loss: 2.1579 - val_acc: 0.2764\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 30s 762us/step - loss: 2.3154 - acc: 0.2596 - val_loss: 1.9833 - val_acc: 0.3101\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 30s 756us/step - loss: 2.3262 - acc: 0.2574 - val_loss: 2.0573 - val_acc: 0.2887\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 30s 754us/step - loss: 2.3151 - acc: 0.2583 - val_loss: 1.9286 - val_acc: 0.3188\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 31s 771us/step - loss: 2.3296 - acc: 0.2591 - val_loss: 2.0238 - val_acc: 0.2992\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 31s 767us/step - loss: 2.3205 - acc: 0.2576 - val_loss: 1.9919 - val_acc: 0.3066\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 31s 772us/step - loss: 2.3097 - acc: 0.2590 - val_loss: 1.9621 - val_acc: 0.3117\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 30s 755us/step - loss: 2.3280 - acc: 0.2568 - val_loss: 1.8979 - val_acc: 0.3262\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 31s 780us/step - loss: 2.3213 - acc: 0.2620 - val_loss: 1.8892 - val_acc: 0.3339\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 31s 774us/step - loss: 2.3131 - acc: 0.2650 - val_loss: 1.9848 - val_acc: 0.2939\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 30s 761us/step - loss: 2.3092 - acc: 0.2633 - val_loss: 1.9581 - val_acc: 0.3258\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 31s 778us/step - loss: 2.3175 - acc: 0.2611 - val_loss: 1.9838 - val_acc: 0.3098\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 31s 772us/step - loss: 2.3172 - acc: 0.2618 - val_loss: 1.9885 - val_acc: 0.3085\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 31s 766us/step - loss: 2.3138 - acc: 0.2625 - val_loss: 1.9166 - val_acc: 0.3188\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 31s 784us/step - loss: 2.3150 - acc: 0.2590 - val_loss: 1.9608 - val_acc: 0.3223\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 31s 778us/step - loss: 2.3162 - acc: 0.2643 - val_loss: 2.0134 - val_acc: 0.3083\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 31s 769us/step - loss: 2.3107 - acc: 0.2607 - val_loss: 1.9324 - val_acc: 0.3156\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 31s 767us/step - loss: 2.3043 - acc: 0.2606 - val_loss: 1.9887 - val_acc: 0.3135\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 31s 766us/step - loss: 2.3105 - acc: 0.2611 - val_loss: 1.9467 - val_acc: 0.3170\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 31s 767us/step - loss: 2.3229 - acc: 0.2589 - val_loss: 2.0107 - val_acc: 0.3080\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 31s 775us/step - loss: 2.3020 - acc: 0.2627 - val_loss: 2.0657 - val_acc: 0.3011\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 31s 766us/step - loss: 2.3193 - acc: 0.2624 - val_loss: 1.9710 - val_acc: 0.3024\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 31s 772us/step - loss: 2.3033 - acc: 0.2585 - val_loss: 1.9575 - val_acc: 0.3154\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 31s 763us/step - loss: 2.3091 - acc: 0.2615 - val_loss: 1.9504 - val_acc: 0.3167\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 31s 779us/step - loss: 2.3123 - acc: 0.2614 - val_loss: 1.9146 - val_acc: 0.3312\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 32s 790us/step - loss: 2.3013 - acc: 0.2633 - val_loss: 1.9921 - val_acc: 0.2897\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 31s 776us/step - loss: 2.3175 - acc: 0.2623 - val_loss: 2.0705 - val_acc: 0.2944\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 32s 795us/step - loss: 2.3061 - acc: 0.2616 - val_loss: 1.9664 - val_acc: 0.3091\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 31s 772us/step - loss: 2.3120 - acc: 0.2610 - val_loss: 1.9599 - val_acc: 0.3093\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 31s 774us/step - loss: 2.2977 - acc: 0.2610 - val_loss: 2.1166 - val_acc: 0.2830\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 31s 781us/step - loss: 2.3155 - acc: 0.2616 - val_loss: 1.8962 - val_acc: 0.3295\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 31s 776us/step - loss: 2.3146 - acc: 0.2606 - val_loss: 1.9826 - val_acc: 0.2938\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 31s 766us/step - loss: 2.3128 - acc: 0.2580 - val_loss: 1.9956 - val_acc: 0.3142\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 31s 764us/step - loss: 2.3010 - acc: 0.2603 - val_loss: 1.9806 - val_acc: 0.2875\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 31s 772us/step - loss: 2.3053 - acc: 0.2624 - val_loss: 2.0042 - val_acc: 0.2973\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 31s 772us/step - loss: 2.3145 - acc: 0.2588 - val_loss: 2.0713 - val_acc: 0.2539\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 31s 776us/step - loss: 2.3081 - acc: 0.2579 - val_loss: 2.0087 - val_acc: 0.2919\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 31s 774us/step - loss: 2.3120 - acc: 0.2597 - val_loss: 1.9544 - val_acc: 0.3034\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 31s 771us/step - loss: 2.3282 - acc: 0.2582 - val_loss: 1.9502 - val_acc: 0.3195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 29s 733us/step - loss: 2.3212 - acc: 0.2589 - val_loss: 1.9796 - val_acc: 0.3068\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 30s 742us/step - loss: 2.3044 - acc: 0.2657 - val_loss: 1.9772 - val_acc: 0.2993\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 30s 742us/step - loss: 2.3106 - acc: 0.2601 - val_loss: 1.9739 - val_acc: 0.3065\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 29s 733us/step - loss: 2.2958 - acc: 0.2625 - val_loss: 1.9506 - val_acc: 0.3206\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 29s 735us/step - loss: 2.3127 - acc: 0.2609 - val_loss: 1.9030 - val_acc: 0.3046\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 28s 691us/step - loss: 2.3113 - acc: 0.2630 - val_loss: 2.0212 - val_acc: 0.2925\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 27s 674us/step - loss: 2.3203 - acc: 0.2609 - val_loss: 1.8848 - val_acc: 0.3232\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 27s 670us/step - loss: 2.3126 - acc: 0.2605 - val_loss: 1.8958 - val_acc: 0.3301\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 27s 678us/step - loss: 2.3116 - acc: 0.2606 - val_loss: 2.1427 - val_acc: 0.2993\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 27s 668us/step - loss: 2.3104 - acc: 0.2635 - val_loss: 1.9559 - val_acc: 0.2929\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 27s 683us/step - loss: 2.3121 - acc: 0.2634 - val_loss: 2.0247 - val_acc: 0.3034\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 27s 683us/step - loss: 2.3217 - acc: 0.2594 - val_loss: 2.0541 - val_acc: 0.2822\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 27s 681us/step - loss: 2.3221 - acc: 0.2583 - val_loss: 1.8481 - val_acc: 0.3388\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 28s 688us/step - loss: 2.3171 - acc: 0.2620 - val_loss: 1.9779 - val_acc: 0.3093\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 27s 682us/step - loss: 2.3082 - acc: 0.2605 - val_loss: 2.0354 - val_acc: 0.3047\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 28s 692us/step - loss: 2.3030 - acc: 0.2643 - val_loss: 1.9514 - val_acc: 0.2941\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 27s 676us/step - loss: 2.3258 - acc: 0.2597 - val_loss: 1.8978 - val_acc: 0.3343\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 27s 681us/step - loss: 2.3090 - acc: 0.2588 - val_loss: 2.0269 - val_acc: 0.2913\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 27s 684us/step - loss: 2.3188 - acc: 0.2645 - val_loss: 1.9537 - val_acc: 0.3237\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 27s 687us/step - loss: 2.3178 - acc: 0.2631 - val_loss: 1.9220 - val_acc: 0.3219\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 27s 685us/step - loss: 2.3152 - acc: 0.2601 - val_loss: 2.0213 - val_acc: 0.3227\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 27s 687us/step - loss: 2.3145 - acc: 0.2607 - val_loss: 1.9923 - val_acc: 0.2899\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 27s 683us/step - loss: 2.3106 - acc: 0.2618 - val_loss: 1.9737 - val_acc: 0.3229\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 27s 683us/step - loss: 2.3096 - acc: 0.2632 - val_loss: 1.8995 - val_acc: 0.3280\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 27s 668us/step - loss: 2.3173 - acc: 0.2628 - val_loss: 1.9976 - val_acc: 0.2972\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 27s 672us/step - loss: 2.3196 - acc: 0.2596 - val_loss: 1.9196 - val_acc: 0.3236\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 27s 663us/step - loss: 2.3106 - acc: 0.2577 - val_loss: 2.0227 - val_acc: 0.3102\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 26s 657us/step - loss: 2.3209 - acc: 0.2607 - val_loss: 1.9521 - val_acc: 0.3069\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 27s 669us/step - loss: 2.3053 - acc: 0.2651 - val_loss: 1.9914 - val_acc: 0.3072\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 27s 665us/step - loss: 2.3042 - acc: 0.2652 - val_loss: 1.9037 - val_acc: 0.3328\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 27s 668us/step - loss: 2.3106 - acc: 0.2588 - val_loss: 2.0116 - val_acc: 0.3185\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 27s 672us/step - loss: 2.3245 - acc: 0.2629 - val_loss: 1.8857 - val_acc: 0.3253\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 27s 667us/step - loss: 2.3030 - acc: 0.2588 - val_loss: 1.8736 - val_acc: 0.3436\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 27s 672us/step - loss: 2.3086 - acc: 0.2620 - val_loss: 2.0043 - val_acc: 0.2815\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 27s 672us/step - loss: 2.3180 - acc: 0.2610 - val_loss: 1.9699 - val_acc: 0.3195\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 27s 667us/step - loss: 2.3098 - acc: 0.2606 - val_loss: 1.9569 - val_acc: 0.2990\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 27s 681us/step - loss: 2.2986 - acc: 0.2634 - val_loss: 1.9411 - val_acc: 0.3125\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 27s 668us/step - loss: 2.3147 - acc: 0.2587 - val_loss: 1.8729 - val_acc: 0.3304\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 27s 679us/step - loss: 2.3112 - acc: 0.2616 - val_loss: 1.9027 - val_acc: 0.3169\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 27s 676us/step - loss: 2.3097 - acc: 0.2596 - val_loss: 1.9192 - val_acc: 0.3191\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 27s 685us/step - loss: 2.2980 - acc: 0.2612 - val_loss: 1.8774 - val_acc: 0.3287\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 27s 665us/step - loss: 2.3120 - acc: 0.2607 - val_loss: 1.8959 - val_acc: 0.3162\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# Compile model and train it.\n",
    "# ...\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(trainVal_data, trainVal_label, batch_size=20, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.4449106979370117\n",
      "Validation accuracy (NORMALIZED): 0.5039000006914138\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation:\n",
    "# ...\n",
    "score = history.history['val_loss'][-1], history.history['val_acc'][-1]\n",
    "\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "-----------------\n",
    "\n",
    "# Training last 2 Fire Modules + classification layers\n",
    "As we could see, the frozen network performed very poorly. By freezing most layers, we do not allow SqueezeNet to adapt its weights to features present in CIFAR-10.\n",
    "\n",
    "Let's try to unfreeze the last two fire modules and train once more. The architecture will be:\n",
    "<img src=\"partFrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2 False\n",
      "conv1 False\n",
      "relu_conv1 False\n",
      "pool1 False\n",
      "fire2/squeeze1x1 False\n",
      "fire2/relu_squeeze1x1 False\n",
      "fire2/expand1x1 False\n",
      "fire2/expand3x3 False\n",
      "fire2/relu_expand1x1 False\n",
      "fire2/relu_expand3x3 False\n",
      "fire2/concat False\n",
      "fire3/squeeze1x1 False\n",
      "fire3/relu_squeeze1x1 False\n",
      "fire3/expand1x1 False\n",
      "fire3/expand3x3 False\n",
      "fire3/relu_expand1x1 False\n",
      "fire3/relu_expand3x3 False\n",
      "fire3/concat False\n",
      "pool3 False\n",
      "fire4/squeeze1x1 False\n",
      "fire4/relu_squeeze1x1 False\n",
      "fire4/expand1x1 False\n",
      "fire4/expand3x3 False\n",
      "fire4/relu_expand1x1 False\n",
      "fire4/relu_expand3x3 False\n",
      "fire4/concat False\n",
      "fire5/squeeze1x1 False\n",
      "fire5/relu_squeeze1x1 False\n",
      "fire5/expand1x1 False\n",
      "fire5/expand3x3 False\n",
      "fire5/relu_expand1x1 False\n",
      "fire5/relu_expand3x3 False\n",
      "fire5/concat False\n",
      "pool5 False\n",
      "fire6/squeeze1x1 False\n",
      "fire6/relu_squeeze1x1 False\n",
      "fire6/expand1x1 False\n",
      "fire6/expand3x3 False\n",
      "fire6/relu_expand1x1 False\n",
      "fire6/relu_expand3x3 False\n",
      "fire6/concat False\n",
      "fire7/squeeze1x1 False\n",
      "fire7/relu_squeeze1x1 False\n",
      "fire7/expand1x1 False\n",
      "fire7/expand3x3 False\n",
      "fire7/relu_expand1x1 False\n",
      "fire7/relu_expand3x3 False\n",
      "fire7/concat False\n",
      "fire8/squeeze1x1 True\n",
      "fire8/relu_squeeze1x1 True\n",
      "fire8/expand1x1 True\n",
      "fire8/expand3x3 True\n",
      "fire8/relu_expand1x1 True\n",
      "fire8/relu_expand3x3 True\n",
      "fire8/concat True\n",
      "fire9/squeeze1x1 True\n",
      "fire9/relu_squeeze1x1 True\n",
      "fire9/expand1x1 True\n",
      "fire9/expand3x3 True\n",
      "fire9/relu_expand1x1 True\n",
      "fire9/relu_expand3x3 True\n",
      "fire9/concat True\n",
      "drop9 True\n",
      "conv10 True\n",
      "relu_conv10 True\n",
      "global_average_pooling2d_3 True\n",
      "loss True\n"
     ]
    }
   ],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "#freeze the mentioned layers\n",
    "# ...\n",
    "for layer in squeezeNetModel.layers[:-19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in squeezeNetModel.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "#Add new classification layers\n",
    "# ...\n",
    "x = squeezeNetModel.layers[-5].output\n",
    "x = Convolution2D(1000, (1, 1), padding='valid', name='new_conv10')(x)\n",
    "x = Activation('relu', name='new_relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='new_loss')(x)\n",
    "\n",
    "#new Model\n",
    "model = Model(squeezeNetModel.inputs, x, name='squeezenet_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "new_conv10 (Conv2D)             (None, 1, 1, 1000)   513000      drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "new_relu_conv10 (Activation)    (None, 1, 1, 1000)   0           new_conv10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 1000)         0           new_relu_conv10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "new_loss (Activation)           (None, 1000)         0           global_average_pooling2d_4[0][0] \n",
      "==================================================================================================\n",
      "Total params: 1,235,496\n",
      "Trainable params: 899,176\n",
      "Non-trainable params: 336,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-20].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 2.7378 - acc: 0.1856 - val_loss: 1.9183 - val_acc: 0.2878\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 2.1501 - acc: 0.2315 - val_loss: 1.7880 - val_acc: 0.3375\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 2.0389 - acc: 0.2617 - val_loss: 1.7314 - val_acc: 0.3441\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.9521 - acc: 0.2943 - val_loss: 1.6789 - val_acc: 0.3756\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.8825 - acc: 0.3168 - val_loss: 1.6574 - val_acc: 0.3714\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.8354 - acc: 0.3370 - val_loss: 1.6194 - val_acc: 0.4078\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.7983 - acc: 0.3515 - val_loss: 1.6182 - val_acc: 0.4090\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.7556 - acc: 0.3735 - val_loss: 1.5896 - val_acc: 0.4182\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.7290 - acc: 0.3847 - val_loss: 1.5982 - val_acc: 0.4257\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.7009 - acc: 0.3936 - val_loss: 1.5203 - val_acc: 0.4529\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.6692 - acc: 0.4086 - val_loss: 1.5111 - val_acc: 0.4589\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.6495 - acc: 0.4169 - val_loss: 1.5067 - val_acc: 0.4624\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.6324 - acc: 0.4266 - val_loss: 1.5090 - val_acc: 0.4485\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.6099 - acc: 0.4365 - val_loss: 1.4962 - val_acc: 0.4629\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.5892 - acc: 0.4403 - val_loss: 1.4810 - val_acc: 0.4656\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.5729 - acc: 0.4470 - val_loss: 1.4761 - val_acc: 0.4711\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.5623 - acc: 0.4525 - val_loss: 1.4686 - val_acc: 0.4772\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.5499 - acc: 0.4560 - val_loss: 1.4547 - val_acc: 0.4797\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.5388 - acc: 0.4607 - val_loss: 1.4626 - val_acc: 0.4777\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.5300 - acc: 0.4644 - val_loss: 1.4640 - val_acc: 0.4715\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.5205 - acc: 0.4689 - val_loss: 1.4367 - val_acc: 0.4879\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.5092 - acc: 0.4716 - val_loss: 1.4519 - val_acc: 0.4894\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.4990 - acc: 0.4745 - val_loss: 1.4230 - val_acc: 0.4946\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.4913 - acc: 0.4775 - val_loss: 1.4292 - val_acc: 0.4944\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.4821 - acc: 0.4835 - val_loss: 1.4337 - val_acc: 0.4902\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.4785 - acc: 0.4821 - val_loss: 1.4222 - val_acc: 0.4950\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4692 - acc: 0.4839 - val_loss: 1.4540 - val_acc: 0.4796\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4667 - acc: 0.4861 - val_loss: 1.4241 - val_acc: 0.4935\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4578 - acc: 0.4889 - val_loss: 1.4133 - val_acc: 0.4996\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4496 - acc: 0.4909 - val_loss: 1.4158 - val_acc: 0.4979\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4466 - acc: 0.4925 - val_loss: 1.4158 - val_acc: 0.4996\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4418 - acc: 0.4933 - val_loss: 1.4078 - val_acc: 0.4975\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4332 - acc: 0.5004 - val_loss: 1.4190 - val_acc: 0.4958\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4284 - acc: 0.5005 - val_loss: 1.4132 - val_acc: 0.4975\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4276 - acc: 0.5019 - val_loss: 1.4089 - val_acc: 0.4983\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.4205 - acc: 0.5045 - val_loss: 1.4066 - val_acc: 0.4975\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.4135 - acc: 0.5053 - val_loss: 1.3946 - val_acc: 0.5007\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.4114 - acc: 0.5062 - val_loss: 1.3987 - val_acc: 0.5028\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.4048 - acc: 0.5077 - val_loss: 1.4092 - val_acc: 0.5047\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 53s 1ms/step - loss: 1.3966 - acc: 0.5111 - val_loss: 1.4186 - val_acc: 0.4954\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3899 - acc: 0.5122 - val_loss: 1.4138 - val_acc: 0.4985\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3888 - acc: 0.5126 - val_loss: 1.4101 - val_acc: 0.5017\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3843 - acc: 0.5156 - val_loss: 1.3912 - val_acc: 0.5073\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3800 - acc: 0.5152 - val_loss: 1.3954 - val_acc: 0.5043\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3747 - acc: 0.5173 - val_loss: 1.3942 - val_acc: 0.5033\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.3729 - acc: 0.5187 - val_loss: 1.4075 - val_acc: 0.4990\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3652 - acc: 0.5217 - val_loss: 1.3961 - val_acc: 0.5058\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3620 - acc: 0.5217 - val_loss: 1.3916 - val_acc: 0.5086\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3582 - acc: 0.5218 - val_loss: 1.3938 - val_acc: 0.5067\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3518 - acc: 0.5248 - val_loss: 1.4044 - val_acc: 0.5040\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3468 - acc: 0.5253 - val_loss: 1.3867 - val_acc: 0.5076\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3471 - acc: 0.5268 - val_loss: 1.3854 - val_acc: 0.5075\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3388 - acc: 0.5270 - val_loss: 1.3937 - val_acc: 0.5054\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.3390 - acc: 0.5282 - val_loss: 1.3984 - val_acc: 0.5067\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3342 - acc: 0.5287 - val_loss: 1.4039 - val_acc: 0.5029\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3293 - acc: 0.5329 - val_loss: 1.4056 - val_acc: 0.5022\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3290 - acc: 0.5328 - val_loss: 1.3924 - val_acc: 0.5083\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.3223 - acc: 0.5333 - val_loss: 1.4104 - val_acc: 0.5032\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 49s 1ms/step - loss: 1.3199 - acc: 0.5333 - val_loss: 1.3850 - val_acc: 0.5104\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.3169 - acc: 0.5350 - val_loss: 1.3897 - val_acc: 0.5096\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 49s 1ms/step - loss: 1.3096 - acc: 0.5384 - val_loss: 1.3906 - val_acc: 0.5119\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.3045 - acc: 0.5384 - val_loss: 1.4286 - val_acc: 0.4957\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.3021 - acc: 0.5399 - val_loss: 1.3969 - val_acc: 0.5119\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2985 - acc: 0.5418 - val_loss: 1.3916 - val_acc: 0.5110\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 49s 1ms/step - loss: 1.2957 - acc: 0.5418 - val_loss: 1.4088 - val_acc: 0.5069\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2924 - acc: 0.5421 - val_loss: 1.3882 - val_acc: 0.5131\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.2846 - acc: 0.5449 - val_loss: 1.4157 - val_acc: 0.5047\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2834 - acc: 0.5449 - val_loss: 1.4028 - val_acc: 0.5090\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2821 - acc: 0.5482 - val_loss: 1.4156 - val_acc: 0.5030\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2819 - acc: 0.5456 - val_loss: 1.3903 - val_acc: 0.5103\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 49s 1ms/step - loss: 1.2712 - acc: 0.5494 - val_loss: 1.4069 - val_acc: 0.5074\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2727 - acc: 0.5507 - val_loss: 1.3943 - val_acc: 0.5076\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2662 - acc: 0.5514 - val_loss: 1.4084 - val_acc: 0.5064\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.2664 - acc: 0.5479 - val_loss: 1.4068 - val_acc: 0.5078\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 49s 1ms/step - loss: 1.2594 - acc: 0.5524 - val_loss: 1.3907 - val_acc: 0.5100\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.2559 - acc: 0.5538 - val_loss: 1.4101 - val_acc: 0.5054\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.2523 - acc: 0.5549 - val_loss: 1.4173 - val_acc: 0.5049\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.2500 - acc: 0.5560 - val_loss: 1.4035 - val_acc: 0.5106\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.2457 - acc: 0.5571 - val_loss: 1.4061 - val_acc: 0.5089\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.2404 - acc: 0.5577 - val_loss: 1.4230 - val_acc: 0.5041\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2396 - acc: 0.5595 - val_loss: 1.3977 - val_acc: 0.5117\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.2384 - acc: 0.5579 - val_loss: 1.4106 - val_acc: 0.5090\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.2359 - acc: 0.5605 - val_loss: 1.4129 - val_acc: 0.5122\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 53s 1ms/step - loss: 1.2289 - acc: 0.5617 - val_loss: 1.4191 - val_acc: 0.5089\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 1.2261 - acc: 0.5643 - val_loss: 1.3989 - val_acc: 0.5094\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 49s 1ms/step - loss: 1.2215 - acc: 0.5646 - val_loss: 1.4316 - val_acc: 0.5053\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 48s 1ms/step - loss: 1.2220 - acc: 0.5626 - val_loss: 1.4059 - val_acc: 0.5098\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 1.2180 - acc: 0.5676 - val_loss: 1.4107 - val_acc: 0.5068\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 80s 2ms/step - loss: 1.2145 - acc: 0.5663 - val_loss: 1.4247 - val_acc: 0.5092\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 83s 2ms/step - loss: 1.2069 - acc: 0.5683 - val_loss: 1.4163 - val_acc: 0.5106\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 1.2078 - acc: 0.5691 - val_loss: 1.4203 - val_acc: 0.5092\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.2065 - acc: 0.5679 - val_loss: 1.4338 - val_acc: 0.5080\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 71s 2ms/step - loss: 1.2004 - acc: 0.5728 - val_loss: 1.4219 - val_acc: 0.5041\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 93s 2ms/step - loss: 1.1968 - acc: 0.5732 - val_loss: 1.4187 - val_acc: 0.5098\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 92s 2ms/step - loss: 1.1941 - acc: 0.5725 - val_loss: 1.4256 - val_acc: 0.5110\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 1.1928 - acc: 0.5733 - val_loss: 1.4076 - val_acc: 0.5095\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 93s 2ms/step - loss: 1.1901 - acc: 0.5756 - val_loss: 1.4194 - val_acc: 0.5047\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 1.1833 - acc: 0.5771 - val_loss: 1.4294 - val_acc: 0.5112\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 1.1817 - acc: 0.5749 - val_loss: 1.4461 - val_acc: 0.5045\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 1.1815 - acc: 0.5739 - val_loss: 1.4449 - val_acc: 0.5039\n"
     ]
    }
   ],
   "source": [
    "#Compile model and train it\n",
    "# ...\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(trainVal_data, trainVal_label, batch_size=20, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.4449106979370117\n",
      "Validation accuracy (NORMALIZED): 0.5039000006914138\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation.\n",
    "score = history.history['val_loss'], history.history['val_acc']\n",
    "print('Validation loss:', score[0][-1])\n",
    "print('Validation accuracy (NORMALIZED):', score[1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "# Tensorboard\n",
    "\n",
    "Tensorboard is a visualization tool for Tensorflow. Among other things, it allows us to monitor the progress of our training, plot metrics per epochs, visualize the architecture's schematics. \n",
    "\n",
    "Just like for Early Stopping, we will use the [Tensorboard callback](https://keras.io/callbacks/#tensorboard) to log the information about our training. An example of usage, would be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Just an example, DON'T RUN! \n",
    "### You will need to change <<LOG_DIR>>\n",
    "import keras.callbacks as callbacks\n",
    "tbCallBack = callbacks.TensorBoard(log_dir = \"./<<LOG_DIR>>\")\n",
    "model.fit(..., callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your training progresses, Keras will log the metrics (e.g., loss, accuracy) to `<<LOG_DIR>>` (**make sure `<<LOG_DIR>>` is a valid directory)**. On your terminal, you will need to run Tensorboard, assign a port and access it via browser (just like jupyter).\n",
    "\n",
    "#### ----> MAKE SURE YOU USE A DIFFERENT PORT FOR JUPYTER AND TENSORBOARD <----\n",
    "\n",
    "### Docker\n",
    "For those using docker, open a new terminal and create a new container (using the same image) running Tensorboard:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p <<port_host>>:<<port_container>>\n",
    "            --volume=<<LOG_DIR>>:<<LOG_DIR>>\n",
    "            --name=<<container_name>> <<docker_image>> \n",
    "            tensorboard --logdir=<<LOG_DIR>> --port=<<port_container>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p 8887:8887\n",
    "            --volume=/your/path/ml2018/:/ml2018\n",
    "            --name=mdc_container_tensorboard mdc-keras:cpu\n",
    "            tensorboard --logdir=/ml2018/logs --port=8887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port_container>>`.\n",
    "\n",
    "### Anaconda\n",
    "$ tensorboard --logdir=<<LOG_DIR>> --port=<<port>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port>>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "\n",
    "# Fine-tuning all layers\n",
    "\n",
    "What if we fine-tune all layers of SqueezeNet?\n",
    "<img src=\"unfrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_2:0\", shape=(?, 32, 32, 3), dtype=float32) at layer \"input_2\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-788b9da99d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#new Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueezeNetModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'squeezenet_new'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 231\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                                          \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                          \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_2:0\", shape=(?, 32, 32, 3), dtype=float32) at layer \"input_2\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = True       #by default they are all trainable, but just for clarification\n",
    "\n",
    "#Add new classification layers\n",
    "# ...\n",
    "\n",
    "#new Model\n",
    "model = Model(squeezeNetModel.inputs, x, name='squeezenet_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "#Compile model\n",
    "# ...\n",
    "\n",
    "#Tensorboard callback\n",
    "#tbCallBack = TensorBoard(log_dir=\"./logs/rafa\", write_graph=True)\n",
    "tbCallBack = TensorBoard(log_dir=\"/TransferLearning/logs/{}\".format(time()), write_graph=True)\n",
    "\n",
    "#Train model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation\n",
    "# ...\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your best model on test\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "Now that we are working on more complex tasks and our trainings are starting to take more time it is usually a good idea to save the trained model from time to time. [Keras has a lot of ways of saving and loading the model](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model), but in this exercise we will use the simplest of them all: `model.save()`. It saves the architecture, the weights, the choice of loss function/optimizer/metrics and even the current state of the training, so you can resume your training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model\n",
    "Once we have our model trained, we can load it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "del model  # Will delete model, only to check if load_model is working\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# evaluate test set again... should give us the same result\n",
    "# ...\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
