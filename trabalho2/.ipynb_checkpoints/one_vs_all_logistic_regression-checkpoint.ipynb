{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_mnist.utils.mnist_reader import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_image, total_train_labels = load_mnist('./fashion_mnist/data/fashion/', kind='train')\n",
    "test_image, test_labels = load_mnist('./fashion_mnist/data/fashion/', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_array_train = np.ones((total_train_image.shape[0], 1), dtype=int)\n",
    "ones_array_test = np.ones((test_image.shape[0], 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_image = np.hstack((ones_array_train,total_train_image))\n",
    "test_image = np.hstack((ones_array_test,test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, validation_image = train_test_split(total_train_image, test_size=0.15, random_state=0)\n",
    "train_labels, validation_labels = train_test_split(total_train_labels, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_onehot = pd.get_dummies(train_labels)\n",
    "validation_labels_onehot = pd.get_dummies(validation_labels)\n",
    "test_labels_onehot = pd.get_dummies(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(z):\n",
    "    return 1 / (1 + tf.exp(-z))\n",
    "\n",
    "def cross_entropy(y, h):\n",
    "    return tf.reduce_mean(-y * tf.log(h) - (1 - y) * tf.log(1 - h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.000001\n",
    "batch = train_image.shape[0]\n",
    "epochs = 200\n",
    "iterations = int(train_image.shape[0]/batch)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(test_image[4][1:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape =([None,28*28+1]))\n",
    "y = tf.placeholder(tf.float32, shape =([None, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = tf.get_variable(\"theta0\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta1 = tf.get_variable(\"theta1\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta2 = tf.get_variable(\"theta2\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta3 = tf.get_variable(\"theta3\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta4 = tf.get_variable(\"theta4\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta5 = tf.get_variable(\"theta5\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta6 = tf.get_variable(\"theta6\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta7 = tf.get_variable(\"theta7\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta8 = tf.get_variable(\"theta8\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta9 = tf.get_variable(\"theta9\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient0 = tf.get_variable(\"gradient0\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient1 = tf.get_variable(\"gradient1\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient2 = tf.get_variable(\"gradient2\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient3 = tf.get_variable(\"gradient3\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient4 = tf.get_variable(\"gradient4\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient5 = tf.get_variable(\"gradient5\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient6 = tf.get_variable(\"gradient6\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient7 = tf.get_variable(\"gradient7\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient8 = tf.get_variable(\"gradient8\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient9 = tf.get_variable(\"gradient9\", shape=(28*28+1,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = tf.matmul(X,theta0)\n",
    "z1 = tf.matmul(X,theta1)\n",
    "z2 = tf.matmul(X,theta2)\n",
    "z3 = tf.matmul(X,theta3)\n",
    "z4 = tf.matmul(X,theta4)\n",
    "z5 = tf.matmul(X,theta5)\n",
    "z6 = tf.matmul(X,theta6)\n",
    "z7 = tf.matmul(X,theta7)\n",
    "z8 = tf.matmul(X,theta8)\n",
    "z9 = tf.matmul(X,theta9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = sigmoid_function(z0)\n",
    "h1 = sigmoid_function(z1)\n",
    "h2 = sigmoid_function(z2)\n",
    "h3 = sigmoid_function(z3)\n",
    "h4 = sigmoid_function(z4)\n",
    "h5 = sigmoid_function(z5)\n",
    "h6 = sigmoid_function(z6)\n",
    "h7 = sigmoid_function(z7)\n",
    "h8 = sigmoid_function(z8)\n",
    "h9 = sigmoid_function(z9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient0_update = gradient0.assign(tf.tensordot(tf.transpose(X), tf.subtract(h0, y),1))\n",
    "gradient1_update = gradient1.assign(tf.tensordot(tf.transpose(X), tf.subtract(h1, y),1))\n",
    "gradient2_update = gradient2.assign(tf.tensordot(tf.transpose(X), tf.subtract(h2, y),1))\n",
    "gradient3_update = gradient3.assign(tf.tensordot(tf.transpose(X), tf.subtract(h3, y),1))\n",
    "gradient4_update = gradient4.assign(tf.tensordot(tf.transpose(X), tf.subtract(h4, y),1))\n",
    "gradient5_update = gradient5.assign(tf.tensordot(tf.transpose(X), tf.subtract(h5, y),1))\n",
    "gradient6_update = gradient6.assign(tf.tensordot(tf.transpose(X), tf.subtract(h6, y),1))\n",
    "gradient7_update = gradient7.assign(tf.tensordot(tf.transpose(X), tf.subtract(h7, y),1))\n",
    "gradient8_update = gradient8.assign(tf.tensordot(tf.transpose(X), tf.subtract(h8, y),1))\n",
    "gradient9_update = gradient9.assign(tf.tensordot(tf.transpose(X), tf.subtract(h9, y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_update = theta0.assign(theta0 - lr * gradient0)\n",
    "theta1_update = theta1.assign(theta1 - lr * gradient1)\n",
    "theta2_update = theta2.assign(theta2 - lr * gradient2)\n",
    "theta3_update = theta3.assign(theta3 - lr * gradient3)\n",
    "theta4_update = theta4.assign(theta4 - lr * gradient4)\n",
    "theta5_update = theta5.assign(theta5 - lr * gradient5)\n",
    "theta6_update = theta6.assign(theta6 - lr * gradient6)\n",
    "theta7_update = theta7.assign(theta7 - lr * gradient7)\n",
    "theta8_update = theta8.assign(theta8 - lr * gradient8)\n",
    "theta9_update = theta9.assign(theta9 - lr * gradient9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0 = cross_entropy(y,h0)\n",
    "loss1 = cross_entropy(y,h1)\n",
    "loss2 = cross_entropy(y,h2)\n",
    "loss3 = cross_entropy(y,h3)\n",
    "loss4 = cross_entropy(y,h4)\n",
    "loss5 = cross_entropy(y,h5)\n",
    "loss6 = cross_entropy(y,h6)\n",
    "loss7 = cross_entropy(y,h7)\n",
    "loss8 = cross_entropy(y,h8)\n",
    "loss9 = cross_entropy(y,h9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_X_norm = tf.image.per_image_standardization(tf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "train_image_norm = sess.run(tf_X_norm, feed_dict={tf_X: train_image})\n",
    "validation_image_norm = sess.run(tf_X_norm, feed_dict={tf_X: validation_image})\n",
    "test_image_norm = sess.run(tf_X_norm, feed_dict={tf_X: test_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train loss 0.8870\n",
      "Validation loss 1.0446\n",
      "Epoch 1\n",
      "Train loss 0.4846\n",
      "Validation loss 0.5961\n",
      "Epoch 2\n",
      "Train loss 0.3369\n",
      "Validation loss 0.3736\n",
      "Epoch 3\n",
      "Train loss 0.2416\n",
      "Validation loss 0.2076\n",
      "Epoch 4\n",
      "Train loss 0.2102\n",
      "Validation loss 0.1654\n",
      "Epoch 5\n",
      "Train loss 0.1991\n",
      "Validation loss 0.1598\n",
      "Epoch 6\n",
      "Train loss 0.1906\n",
      "Validation loss 0.1553\n",
      "Epoch 7\n",
      "Train loss 0.1842\n",
      "Validation loss 0.1516\n",
      "Epoch 8\n",
      "Train loss 0.1784\n",
      "Validation loss 0.1485\n",
      "Epoch 9\n",
      "Train loss 0.1740\n",
      "Validation loss 0.1459\n",
      "Epoch 10\n",
      "Train loss 0.1697\n",
      "Validation loss 0.1435\n",
      "Epoch 11\n",
      "Train loss 0.1663\n",
      "Validation loss 0.1415\n",
      "Epoch 12\n",
      "Train loss 0.1629\n",
      "Validation loss 0.1397\n",
      "Epoch 13\n",
      "Train loss 0.1601\n",
      "Validation loss 0.1381\n",
      "Epoch 14\n",
      "Train loss 0.1573\n",
      "Validation loss 0.1366\n",
      "Epoch 15\n",
      "Train loss 0.1550\n",
      "Validation loss 0.1353\n",
      "Epoch 16\n",
      "Train loss 0.1528\n",
      "Validation loss 0.1341\n",
      "Epoch 17\n",
      "Train loss 0.1508\n",
      "Validation loss 0.1330\n",
      "Epoch 18\n",
      "Train loss 0.1489\n",
      "Validation loss 0.1320\n",
      "Epoch 19\n",
      "Train loss 0.1472\n",
      "Validation loss 0.1311\n",
      "Epoch 20\n",
      "Train loss 0.1456\n",
      "Validation loss 0.1303\n",
      "Epoch 21\n",
      "Train loss 0.1441\n",
      "Validation loss 0.1295\n",
      "Epoch 22\n",
      "Train loss 0.1428\n",
      "Validation loss 0.1288\n",
      "Epoch 23\n",
      "Train loss 0.1415\n",
      "Validation loss 0.1281\n",
      "Epoch 24\n",
      "Train loss 0.1403\n",
      "Validation loss 0.1274\n",
      "Epoch 25\n",
      "Train loss 0.1391\n",
      "Validation loss 0.1268\n",
      "Epoch 26\n",
      "Train loss 0.1381\n",
      "Validation loss 0.1263\n",
      "Epoch 27\n",
      "Train loss 0.1371\n",
      "Validation loss 0.1258\n",
      "Epoch 28\n",
      "Train loss 0.1361\n",
      "Validation loss 0.1253\n",
      "Epoch 29\n",
      "Train loss 0.1352\n",
      "Validation loss 0.1248\n",
      "Epoch 30\n",
      "Train loss 0.1343\n",
      "Validation loss 0.1243\n",
      "Epoch 31\n",
      "Train loss 0.1335\n",
      "Validation loss 0.1239\n",
      "Epoch 32\n",
      "Train loss 0.1327\n",
      "Validation loss 0.1235\n",
      "Epoch 33\n",
      "Train loss 0.1320\n",
      "Validation loss 0.1231\n",
      "Epoch 34\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_epoch_loss_list = []\n",
    "validation_epoch_loss0_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch_accumulate_loss = 0\n",
    "    \n",
    "    print('Epoch {}'.format(i))\n",
    "    for j in range(iterations):\n",
    "        index = j*batch\n",
    "\n",
    "\n",
    "        _,_, train_loss_value0 = sess.run([gradient0_update,theta0_update, cross_entropy(y, h0)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][0][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value1 = sess.run([gradient1_update,theta1_update, cross_entropy(y, h1)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][1][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value2 = sess.run([gradient2_update,theta2_update, cross_entropy(y, h2)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][2][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value3 = sess.run([gradient3_update,theta3_update, cross_entropy(y, h3)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][3][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value4 = sess.run([gradient4_update,theta4_update, cross_entropy(y, h4)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][4][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value5 = sess.run([gradient5_update,theta5_update, cross_entropy(y, h5)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][5][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value6 = sess.run([gradient6_update,theta6_update, cross_entropy(y, h6)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][6][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value7 = sess.run([gradient7_update,theta7_update, cross_entropy(y, h7)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][7][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value8 = sess.run([gradient8_update,theta8_update, cross_entropy(y, h8)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][8][index:index+batch].values.reshape(batch,1)})\n",
    "        _,_, train_loss_value9 = sess.run([gradient9_update,theta9_update, cross_entropy(y, h9)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][9][index:index+batch].values.reshape(batch,1)})\n",
    "\n",
    "        batch_accumulate_loss += train_loss_value0\n",
    "        batch_accumulate_loss += train_loss_value1\n",
    "        batch_accumulate_loss += train_loss_value2\n",
    "        batch_accumulate_loss += train_loss_value3\n",
    "        batch_accumulate_loss += train_loss_value4\n",
    "        batch_accumulate_loss += train_loss_value5\n",
    "        batch_accumulate_loss += train_loss_value6\n",
    "        batch_accumulate_loss += train_loss_value7\n",
    "        batch_accumulate_loss += train_loss_value8\n",
    "        batch_accumulate_loss += train_loss_value9\n",
    "        \n",
    "    train_epoch_loss = batch_accumulate_loss/(iterations*num_classes)\n",
    "    train_epoch_loss_list.append(train_epoch_loss)\n",
    "    \n",
    "    validation_epoch_loss0 = sess.run(cross_entropy(y,h0), feed_dict={X:validation_image_norm, y:validation_labels_onehot.iloc[:][0].values.reshape(-1,1)})\n",
    "    validation_epoch_loss0_list.append(validation_epoch_loss0)\n",
    "\n",
    "    print('Train loss {0:.4f}'.format(train_epoch_loss))\n",
    "    print('Validation loss {0:.4f}'.format(validation_epoch_loss0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Cost')\n",
    "plt.plot(range(len(train_epoch_loss_list)),train_epoch_loss_list, '-bx')\n",
    "plt.plot(range(len(validation_epoch_loss_list)),validation_epoch_loss_list, '-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_values0 = sess.run(h0, feed_dict={X:test_image_norm})\n",
    "h_values1 = sess.run(h1, feed_dict={X:test_image_norm})\n",
    "h_values2 = sess.run(h2, feed_dict={X:test_image_norm})\n",
    "h_values3 = sess.run(h3, feed_dict={X:test_image_norm})\n",
    "h_values4 = sess.run(h4, feed_dict={X:test_image_norm})\n",
    "h_values5 = sess.run(h5, feed_dict={X:test_image_norm})\n",
    "h_values6 = sess.run(h6, feed_dict={X:test_image_norm})\n",
    "h_values7 = sess.run(h7, feed_dict={X:test_image_norm})\n",
    "h_values8 = sess.run(h8, feed_dict={X:test_image_norm})\n",
    "h_values9 = sess.run(h9, feed_dict={X:test_image_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = np.maximum.reduce([h_values0,h_values1,h_values2,h_values3,h_values4,h_values5,h_values6,h_values7,h_values8,h_values9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_index = h_values0 >= max_values\n",
    "h1_index = h_values1 >= max_values\n",
    "h2_index = h_values2 >= max_values\n",
    "h3_index = h_values3 >= max_values \n",
    "h4_index = h_values4 >= max_values\n",
    "h5_index = h_values5 >= max_values \n",
    "h6_index = h_values6 >= max_values\n",
    "h7_index = h_values7 >= max_values\n",
    "h8_index = h_values8 >= max_values\n",
    "h9_index = h_values9 >= max_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels = np.zeros_like(h0_index,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels[h1_index==1]=1\n",
    "result_labels[h2_index==1]=2\n",
    "result_labels[h3_index==1]=3\n",
    "result_labels[h4_index==1]=4\n",
    "result_labels[h5_index==1]=5\n",
    "result_labels[h6_index==1]=6\n",
    "result_labels[h7_index==1]=7\n",
    "result_labels[h8_index==1]=8\n",
    "result_labels[h9_index==1]=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_labels,result_labels.ravel(), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix(test_labels,result_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
