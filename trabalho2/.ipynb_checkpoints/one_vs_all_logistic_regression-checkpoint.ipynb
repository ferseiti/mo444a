{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import os\n",
    "import copy\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_mnist.utils.mnist_reader import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_image, total_train_labels = load_mnist('./fashion_mnist/data/fashion/', kind='train')\n",
    "test_image, test_labels = load_mnist('./fashion_mnist/data/fashion/', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, validation_image = train_test_split(total_train_image, test_size=0.15, random_state=0)\n",
    "train_labels, validation_labels = train_test_split(total_train_labels, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_onehot = pd.get_dummies(train_labels)\n",
    "validation_labels_onehot = pd.get_dummies(validation_labels)\n",
    "test_labels_onehot = pd.get_dummies(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(z):\n",
    "    return 1 / (1 + tf.exp(-z))\n",
    "\n",
    "def cross_entropy(y, h):\n",
    "    return tf.reduce_mean(-y * tf.log(h) - (1 - y) * tf.log(1 - h))\n",
    "\n",
    "def gradient_update(gradient,h):\n",
    "    return gradient.assign(tf.tensordot(tf.transpose(X), tf.subtract(h, y),1)/tf.cast(tf.shape(h)[0], dtype=tf.float32))\n",
    "\n",
    "def theta_update(theta, gradient):\n",
    "    return theta.assign(theta - lr * gradient)\n",
    "\n",
    "def pred_y(X,theta):\n",
    "    return tf.tensordot(X,theta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch = train_image.shape[0]\n",
    "epochs = 100\n",
    "iterations = int(train_image.shape[0]/batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(test_image[4].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape =([None,28*28]))\n",
    "y = tf.placeholder(tf.float32, shape =([None, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = tf.get_variable(\"theta0\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta1 = tf.get_variable(\"theta1\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta2 = tf.get_variable(\"theta2\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta3 = tf.get_variable(\"theta3\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta4 = tf.get_variable(\"theta4\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta5 = tf.get_variable(\"theta5\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta6 = tf.get_variable(\"theta6\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta7 = tf.get_variable(\"theta7\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta8 = tf.get_variable(\"theta8\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "theta9 = tf.get_variable(\"theta9\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient0 = tf.get_variable(\"gradient0\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient1 = tf.get_variable(\"gradient1\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient2 = tf.get_variable(\"gradient2\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient3 = tf.get_variable(\"gradient3\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient4 = tf.get_variable(\"gradient4\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient5 = tf.get_variable(\"gradient5\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient6 = tf.get_variable(\"gradient6\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient7 = tf.get_variable(\"gradient7\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient8 = tf.get_variable(\"gradient8\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "gradient9 = tf.get_variable(\"gradient9\", shape=(28*28,1), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = pred_y(X,theta0)\n",
    "z1 = pred_y(X,theta1)\n",
    "z2 = pred_y(X,theta2)\n",
    "z3 = pred_y(X,theta3)\n",
    "z4 = pred_y(X,theta4)\n",
    "z5 = pred_y(X,theta5)\n",
    "z6 = pred_y(X,theta6)\n",
    "z7 = pred_y(X,theta7)\n",
    "z8 = pred_y(X,theta8)\n",
    "z9 = pred_y(X,theta9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = sigmoid_function(z0)\n",
    "h1 = sigmoid_function(z1)\n",
    "h2 = sigmoid_function(z2)\n",
    "h3 = sigmoid_function(z3)\n",
    "h4 = sigmoid_function(z4)\n",
    "h5 = sigmoid_function(z5)\n",
    "h6 = sigmoid_function(z6)\n",
    "h7 = sigmoid_function(z7)\n",
    "h8 = sigmoid_function(z8)\n",
    "h9 = sigmoid_function(z9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_update = theta_update(theta0,gradient0)\n",
    "theta1_update = theta_update(theta1,gradient1)\n",
    "theta2_update = theta_update(theta2,gradient2)\n",
    "theta3_update = theta_update(theta3,gradient3)\n",
    "theta4_update = theta_update(theta4,gradient4)\n",
    "theta5_update = theta_update(theta5,gradient5)\n",
    "theta6_update = theta_update(theta6,gradient6)\n",
    "theta7_update = theta_update(theta7,gradient7)\n",
    "theta8_update = theta_update(theta8,gradient8)\n",
    "theta9_update = theta_update(theta9,gradient9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0 = cross_entropy(y,h0)\n",
    "loss1 = cross_entropy(y,h1)\n",
    "loss2 = cross_entropy(y,h2)\n",
    "loss3 = cross_entropy(y,h3)\n",
    "loss4 = cross_entropy(y,h4)\n",
    "loss5 = cross_entropy(y,h5)\n",
    "loss6 = cross_entropy(y,h6)\n",
    "loss7 = cross_entropy(y,h7)\n",
    "loss8 = cross_entropy(y,h8)\n",
    "loss9 = cross_entropy(y,h9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_X_norm = tf.image.per_image_standardization(tf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "train_image_norm = sess.run(tf_X_norm, feed_dict={tf_X: train_image})\n",
    "validation_image_norm = sess.run(tf_X_norm, feed_dict={tf_X: validation_image})\n",
    "test_image_norm = sess.run(tf_X_norm, feed_dict={tf_X: test_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train loss 1.0014\n",
      "Validation loss 0.6122\n",
      "Epoch 1\n",
      "Train loss 0.5884\n",
      "Validation loss 0.4944\n",
      "Epoch 2\n",
      "Train loss 0.4532\n",
      "Validation loss 0.4247\n",
      "Epoch 3\n",
      "Train loss 0.3957\n",
      "Validation loss 0.3789\n",
      "Epoch 4\n",
      "Train loss 0.3609\n",
      "Validation loss 0.3463\n",
      "Epoch 5\n",
      "Train loss 0.3364\n",
      "Validation loss 0.3216\n",
      "Epoch 6\n",
      "Train loss 0.3177\n",
      "Validation loss 0.3022\n",
      "Epoch 7\n",
      "Train loss 0.3026\n",
      "Validation loss 0.2865\n",
      "Epoch 8\n",
      "Train loss 0.2902\n",
      "Validation loss 0.2735\n",
      "Epoch 9\n",
      "Train loss 0.2797\n",
      "Validation loss 0.2626\n",
      "Epoch 10\n",
      "Train loss 0.2707\n",
      "Validation loss 0.2533\n",
      "Epoch 11\n",
      "Train loss 0.2628\n",
      "Validation loss 0.2452\n",
      "Epoch 12\n",
      "Train loss 0.2559\n",
      "Validation loss 0.2382\n",
      "Epoch 13\n",
      "Train loss 0.2498\n",
      "Validation loss 0.2320\n",
      "Epoch 14\n",
      "Train loss 0.2443\n",
      "Validation loss 0.2264\n",
      "Epoch 15\n",
      "Train loss 0.2394\n",
      "Validation loss 0.2215\n",
      "Epoch 16\n",
      "Train loss 0.2349\n",
      "Validation loss 0.2170\n",
      "Epoch 17\n",
      "Train loss 0.2308\n",
      "Validation loss 0.2129\n",
      "Epoch 18\n",
      "Train loss 0.2270\n",
      "Validation loss 0.2092\n",
      "Epoch 19\n",
      "Train loss 0.2235\n",
      "Validation loss 0.2057\n",
      "Epoch 20\n",
      "Train loss 0.2203\n",
      "Validation loss 0.2026\n",
      "Epoch 21\n",
      "Train loss 0.2173\n",
      "Validation loss 0.1997\n",
      "Epoch 22\n",
      "Train loss 0.2145\n",
      "Validation loss 0.1970\n",
      "Epoch 23\n",
      "Train loss 0.2119\n",
      "Validation loss 0.1944\n",
      "Epoch 24\n",
      "Train loss 0.2094\n",
      "Validation loss 0.1921\n",
      "Epoch 25\n",
      "Train loss 0.2071\n",
      "Validation loss 0.1899\n",
      "Epoch 26\n",
      "Train loss 0.2049\n",
      "Validation loss 0.1878\n",
      "Epoch 27\n",
      "Train loss 0.2028\n",
      "Validation loss 0.1858\n",
      "Epoch 28\n",
      "Train loss 0.2008\n",
      "Validation loss 0.1840\n",
      "Epoch 29\n",
      "Train loss 0.1989\n",
      "Validation loss 0.1822\n",
      "Epoch 30\n",
      "Train loss 0.1971\n",
      "Validation loss 0.1806\n",
      "Epoch 31\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_epoch_loss_list = []\n",
    "validation_epoch_loss0_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch_accumulate_loss = 0\n",
    "    \n",
    "    print('Epoch {}'.format(i))\n",
    "    for j in range(iterations):\n",
    "        index = j*batch\n",
    "\n",
    "        theta0_value = sess.run(theta_update(theta0,gradient0))\n",
    "        theta1_value = sess.run(theta_update(theta1,gradient1))\n",
    "        theta2_value = sess.run(theta_update(theta2,gradient2))\n",
    "        theta3_value = sess.run(theta_update(theta3,gradient3))\n",
    "        theta4_value = sess.run(theta_update(theta4,gradient4))\n",
    "        theta5_value = sess.run(theta_update(theta5,gradient5))\n",
    "        theta6_value = sess.run(theta_update(theta6,gradient6))\n",
    "        theta7_value = sess.run(theta_update(theta7,gradient7))\n",
    "        theta8_value = sess.run(theta_update(theta8,gradient8))\n",
    "        theta9_value = sess.run(theta_update(theta9,gradient9))\n",
    "\n",
    "        y_values0, h_values0, gradient_values0, train_loss_value0 = sess.run([y, h0, gradient_update(gradient0, h0), cross_entropy(y, h0)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][0][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values1, h_values1, gradient_values1, train_loss_value1 = sess.run([y, h1, gradient_update(gradient1, h1), cross_entropy(y, h1)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][1][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values2, h_values2, gradient_values2, train_loss_value2 = sess.run([y, h2, gradient_update(gradient2, h2), cross_entropy(y, h2)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][2][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values3, h_values3, gradient_values3, train_loss_value3 = sess.run([y, h3, gradient_update(gradient3, h3), cross_entropy(y, h3)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][3][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values4, h_values4, gradient_values4, train_loss_value4 = sess.run([y, h4, gradient_update(gradient4, h4), cross_entropy(y, h4)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][4][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values5, h_values5, gradient_values5, train_loss_value5 = sess.run([y, h5, gradient_update(gradient5, h5), cross_entropy(y, h5)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][5][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values6, h_values6, gradient_values6, train_loss_value6 = sess.run([y, h6, gradient_update(gradient6, h6), cross_entropy(y, h6)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][6][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values7, h_values7, gradient_values7, train_loss_value7 = sess.run([y, h7, gradient_update(gradient7, h7), cross_entropy(y, h7)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][7][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values8, h_values8, gradient_values8, train_loss_value8 = sess.run([y, h8, gradient_update(gradient8, h8), cross_entropy(y, h8)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][8][index:index+batch].values.reshape(batch,1)})\n",
    "        y_values9, h_values9, gradient_values9, train_loss_value9 = sess.run([y, h9, gradient_update(gradient9, h9), cross_entropy(y, h9)], feed_dict={X:train_image_norm[index:index+batch], y:train_labels_onehot.iloc[:][9][index:index+batch].values.reshape(batch,1)})\n",
    "\n",
    "        batch_accumulate_loss += train_loss_value0\n",
    "        batch_accumulate_loss += train_loss_value1\n",
    "        batch_accumulate_loss += train_loss_value2\n",
    "        batch_accumulate_loss += train_loss_value3\n",
    "        batch_accumulate_loss += train_loss_value4\n",
    "        batch_accumulate_loss += train_loss_value5\n",
    "        batch_accumulate_loss += train_loss_value6\n",
    "        batch_accumulate_loss += train_loss_value7\n",
    "        batch_accumulate_loss += train_loss_value8\n",
    "        batch_accumulate_loss += train_loss_value9\n",
    "        \n",
    "    train_epoch_loss = batch_accumulate_loss/(iterations*10)\n",
    "    train_epoch_loss_list.append(train_epoch_loss)\n",
    "    \n",
    "    validation_epoch_loss0 = sess.run(loss0, feed_dict={X:validation_image_norm, y:validation_labels_onehot.iloc[:][0].values.reshape(-1,1)})\n",
    "    validation_epoch_loss0_list.append(validation_epoch_loss0)\n",
    "\n",
    "    print('Train loss {0:.4f}'.format(train_epoch_loss))\n",
    "    print('Validation loss {0:.4f}'.format(validation_epoch_loss0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Cost')\n",
    "plt.plot(range(len(train_epoch_loss_list)),train_epoch_loss_list, '-bx')\n",
    "plt.plot(range(len(validation_epoch_loss_list)),validation_epoch_loss_list, '-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_values0 = sess.run(h0, feed_dict={X:test_image_norm})\n",
    "h_values1 = sess.run(h1, feed_dict={X:test_image_norm})\n",
    "h_values2 = sess.run(h2, feed_dict={X:test_image_norm})\n",
    "h_values3 = sess.run(h3, feed_dict={X:test_image_norm})\n",
    "h_values4 = sess.run(h4, feed_dict={X:test_image_norm})\n",
    "h_values5 = sess.run(h5, feed_dict={X:test_image_norm})\n",
    "h_values6 = sess.run(h6, feed_dict={X:test_image_norm})\n",
    "h_values7 = sess.run(h7, feed_dict={X:test_image_norm})\n",
    "h_values8 = sess.run(h8, feed_dict={X:test_image_norm})\n",
    "h_values9 = sess.run(h9, feed_dict={X:test_image_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = np.maximum.reduce([h_values0,h_values1,h_values2,h_values3,h_values4,h_values5,h_values6,h_values7,h_values8,h_values9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_index = h_values0 >= max_values\n",
    "h1_index = h_values1 >= max_values\n",
    "h2_index = h_values2 >= max_values\n",
    "h3_index = h_values3 >= max_values \n",
    "h4_index = h_values4 >= max_values\n",
    "h5_index = h_values5 >= max_values \n",
    "h6_index = h_values6 >= max_values\n",
    "h7_index = h_values7 >= max_values\n",
    "h8_index = h_values8 >= max_values\n",
    "h9_index = h_values9 >= max_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels = np.zeros_like(h0_index,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels[h1_index==1]=1\n",
    "result_labels[h2_index==1]=2\n",
    "result_labels[h3_index==1]=3\n",
    "result_labels[h4_index==1]=4\n",
    "result_labels[h5_index==1]=5\n",
    "result_labels[h6_index==1]=6\n",
    "result_labels[h7_index==1]=7\n",
    "result_labels[h8_index==1]=8\n",
    "result_labels[h9_index==1]=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_labels,result_labels.ravel(), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix(test_labels,result_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
